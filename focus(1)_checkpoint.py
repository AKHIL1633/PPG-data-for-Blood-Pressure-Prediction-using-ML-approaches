# -*- coding: utf-8 -*-
"""focus(1)_checkpoint.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B1YdXK2qVVxyUPqpz9wvjThiUyvWQqPy
"""

import pandas as pd
df=pd.read_csv('file.csv')
df.head()

subset_data = df.iloc[:, 1:-2]
print(subset_data)

subset_data.head()

print(subset_data.shape)

reloaded_data = pd.read_csv("subset_data.csv")
print(reloaded_data.head())
print(reloaded_data.shape)

!pip install pywavelets

import numpy as np
import pandas as pd
import pywt
import matplotlib.pyplot as plt

# Load the dataset
reloaded_data = pd.read_csv("subset_data.csv")
print(reloaded_data.head())
print(reloaded_data.shape)

# Function to apply DWT (Discrete Wavelet Transform) for noise reduction
def apply_wavelet_on_row(signal):
    # Apply DWT (you can choose the wavelet and level)
    coeffs = pywt.wavedec(signal, 'db4', level=5)  # db4 is Daubechies wavelet

    # Thresholding the coefficients for noise removal (example threshold)
    thresholded_coeffs = [np.where(np.abs(coeff) > 0.05, coeff, 0) for coeff in coeffs]

    # Reconstruct the signal from thresholded coefficients
    denoised_signal = pywt.waverec(thresholded_coeffs, 'db4')

    return denoised_signal

# Plotting function for comparison
def plot_comparison(original_signal, denoised_signal, index=0):
    plt.figure(figsize=(10, 6))
    plt.subplot(2, 1, 1)
    plt.plot(original_signal, label="Original Signal")
    plt.title(f"Original Signal - Row {index}")
    plt.legend()

    plt.subplot(2, 1, 2)
    plt.plot(denoised_signal, label="Denoised Signal", color='orange')
    plt.title(f"Denoised Signal - Row {index}")
    plt.legend()

    plt.tight_layout()
    plt.show()

# Apply DWT to a sample row (you can choose any row from your dataset)
sample_index = 0  # Change this index to visualize other rows
sample_row = reloaded_data.iloc[sample_index]

# Apply wavelet denoising
denoised_sample = apply_wavelet_on_row(sample_row)

# Plot the original and denoised signals
plot_comparison(sample_row, denoised_sample, sample_index)

# Apply DWT to all rows and store the denoised signals (optional)
denoised_data = []
for index, row in reloaded_data.iterrows():
    signal = row.to_numpy()
    denoised_signal = apply_wavelet_on_row(signal)
    denoised_data.append(denoised_signal[:len(signal)])  # Trim to the original length if needed

# Convert denoised data to DataFrame
denoised_df = pd.DataFrame(denoised_data, columns=reloaded_data.columns)

# Save the denoised dataset
denoised_df.to_csv("denoised_subset_data.csv", index=False)
print("Denoised data has been written to 'denoised_subset_data.csv'")

# Load the original and denoised datasets
original_data = pd.read_csv("subset_data.csv")
denoised_data = pd.read_csv("denoised_subset_data.csv")

# Get Row 1 of each dataset (index 1)
original_row_1 = original_data.iloc[1]
denoised_row_1 = denoised_data.iloc[1]

# Calculate the difference
difference_row_1 = original_row_1 - denoised_row_1
print("Difference in Row 1 values:")
print(difference_row_1)

denoised_data.head()

original_data.head()

#denoised_df = reloaded_data.apply(lambda row: apply_wavelet_on_row(row.to_numpy()), axis=1, result_type="expand")

"""# plot before after normalised the signal"""

import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
subset_data = pd.read_csv("subset_data.csv")

# Apply Min-Max normalization (from the original code you provided)
subset_data_normalized = subset_data.apply(lambda x: (x - x.min()) / (x.max() - x.min()), axis=0)

# Print the first few rows of the normalized data
print(subset_data_normalized.head())

# Plotting the comparison of the original and normalized data
# Choose a specific row (signal) to visualize, for example, the first row
row_index = 0  # Change this index to visualize other rows

# Extract the original and normalized signal for this row
original_signal = subset_data.iloc[row_index, :-2]  # Exclude the last two label columns
normalized_signal = subset_data_normalized.iloc[row_index, :-2]  # Exclude the last two label columns

# Create the plot
plt.figure(figsize=(12, 6))

# Plot original signal
plt.subplot(2, 1, 1)
plt.plot(original_signal, label="Original Signal", color="blue")
plt.title(f"Original Signal - Row {row_index}")
plt.xlabel("Time (seconds)")
plt.ylabel("Signal Amplitude")
plt.legend()

# Plot normalized signal
plt.subplot(2, 1, 2)
plt.plot(normalized_signal, label="Normalized Signal", color="orange")
plt.title(f"Normalized Signal - Row {row_index}")
plt.xlabel("Time (seconds)")
plt.ylabel("Normalized Amplitude")
plt.legend()

plt.tight_layout()
plt.show()

subset_data_normalized.to_csv('subset_data_normalized.csv', index=False)



!pip install EMD-signal

import numpy as np
import pandas as pd
from scipy.signal import find_peaks
import matplotlib.pyplot as plt

# Assuming `subset_data_normalized` is already loaded
ppg_signal = subset_data_normalized.iloc[0, :-2].values  # Use only the signal columns, exclude labels

# Step 1: Normalize the PPG waveform to range 0-1 for amplitude
min_val = np.min(ppg_signal)
max_val = np.max(ppg_signal)
ppg_signal_normalized = -1 + 2 * (ppg_signal - min_val) / (max_val - min_val)

# Step 2: Align the time axis to range from 0 to 1 (normalize x-axis as well)
time_axis = np.linspace(0, 1, len(ppg_signal_normalized))

# Step 3: Detect peaks on the normalized signal
systolic_peaks, _ = find_peaks(ppg_signal_normalized, height=0.6, distance=30)
diastolic_peaks, _ = find_peaks(-ppg_signal_normalized, height=-0.3, distance=30)

# Detect dicrotic notch (minimum between systolic peaks)
dicrotic_notches = []
for i in range(len(systolic_peaks) - 1):
    start, end = systolic_peaks[i], systolic_peaks[i + 1]
    if start < end:
        dicrotic_notch = np.argmin(ppg_signal_normalized[start:end]) + start
        dicrotic_notches.append(dicrotic_notch)

# Step 4: Plot with annotations

plt.figure(figsize=(12, 6))
plt.plot(time_axis, ppg_signal_normalized, label="Normalized PPG Signal", color='purple')
plt.axhline(0, color='brown', linestyle='--', linewidth=2, label="x=0 Line")


# Mark Systolic Peaks
plt.plot(time_axis[systolic_peaks], ppg_signal_normalized[systolic_peaks], "ro", label="Systolic Peaks")

# Mark Diastolic Peaks
plt.plot(time_axis[diastolic_peaks], ppg_signal_normalized[diastolic_peaks], "go", label="Diastolic Peaks")

# Mark Dicrotic Notches
plt.plot(time_axis[dicrotic_notches], ppg_signal_normalized[dicrotic_notches], "bo", label="Dicrotic Notches")

# Add annotations for each detected feature
for peak in systolic_peaks:
    plt.annotate("Systolic Peak", (time_axis[peak], ppg_signal_normalized[peak]),
                 textcoords="offset points", xytext=(0,10), ha='center', color='red')

for peak in diastolic_peaks:
    plt.annotate("Diastolic Peak", (time_axis[peak], ppg_signal_normalized[peak]),
                 textcoords="offset points", xytext=(0,10), ha='center', color='green')

for notch in dicrotic_notches:
    plt.annotate("Dicrotic Notch", (time_axis[notch], ppg_signal_normalized[notch]),
                 textcoords="offset points", xytext=(0,-15), ha='center', color='blue')

# Amplitude Difference (Augmentation)
if len(systolic_peaks) > 0 and len(diastolic_peaks) > 0:
    amplitude_diff = ppg_signal_normalized[systolic_peaks[0]] - ppg_signal_normalized[diastolic_peaks[0]]
    plt.annotate('', xy=(time_axis[systolic_peaks[0]], ppg_signal_normalized[systolic_peaks[0]]),
                 xytext=(time_axis[diastolic_peaks[0]], ppg_signal_normalized[diastolic_peaks[0]]),
                 arrowprops=dict(arrowstyle='<->', color='gray'))
    plt.text((time_axis[systolic_peaks[0]] + time_axis[diastolic_peaks[0]]) / 2,
             (ppg_signal_normalized[systolic_peaks[0]] + ppg_signal_normalized[diastolic_peaks[0]]) / 2,
             f'Amplitude Diff\n{amplitude_diff:.2f}', ha='center', color='gray')

# Time Difference (Systolic to Dicrotic Notch)
if len(systolic_peaks) > 0 and len(dicrotic_notches) > 0:
    time_diff = time_axis[dicrotic_notches[0]] - time_axis[systolic_peaks[0]]
    plt.annotate('', xy=(time_axis[systolic_peaks[0]], 0), xytext=(time_axis[dicrotic_notches[0]], 0),
                 arrowprops=dict(arrowstyle='<->', color='orange'))
    plt.text((time_axis[systolic_peaks[0]] + time_axis[dicrotic_notches[0]]) / 2, -0.05,
             f'Time Diff\n{time_diff:.2f}', ha='center', color='orange')

# Add legend and labels
plt.title("Annotated PPG Signal - Row 0")
plt.xlabel("Normalized Time")
plt.ylabel("Normalized Amplitude")
plt.legend()
plt.grid(True)
plt.show()

from scipy.signal import savgol_filter, butter, filtfilt
subset_data_normalized=pd.read_csv("subset_data_normalized.csv")

# Bandpass Filter Function
def bandpass_filter(signal, lowcut, highcut, fs, order=4):
    nyquist = 0.45 * fs
    low = lowcut / nyquist
    high = highcut / nyquist
    b, a = butter(order, [low, high], btype='band')
    filtered_signal = filtfilt(b, a, signal)
    return filtered_signal

# Adaptive Wavelet Denoising Function
def adaptive_wavelet_denoising(signal, wavelet='coif5', level=2, threshold_method='soft'):
    # Decompose the signal into wavelet coefficients
    coeffs = pywt.wavedec(signal, wavelet, level=level)

    # Calculate adaptive thresholds for each level
    thresholded_coeffs = [coeffs[0]]  # Keep approximation coefficients
    for i in range(1, len(coeffs)):
        sigma = np.median(np.abs(coeffs[i])) / 0.6745  # Robust estimate of noise level
        threshold = sigma * np.sqrt(2 * np.log(len(signal)))  # Adaptive threshold
        thresholded_coeff = pywt.threshold(coeffs[i], threshold, mode=threshold_method)
        thresholded_coeffs.append(thresholded_coeff)

    # Reconstruct the signal from the thresholded coefficients
    denoised_signal = pywt.waverec(thresholded_coeffs, wavelet)
    return denoised_signal

# Apply adaptive wavelet denoising to the PPG signal
ppg_signal = subset_data_normalized.iloc[0, :-2].values  # Exclude the last two columns
ppg_denoised = adaptive_wavelet_denoising(ppg_signal, wavelet='coif5', level=1, threshold_method='soft')

# Apply Bandpass Filter to remove unwanted frequencies
# Assume the signal is sampled at 100 Hz (fs = 100), adjust as needed
fs = 140
lowcut = 0.6  # Lower frequency in Hz
highcut = 11.5  # Higher frequency in Hz
ppg_filtered = bandpass_filter(ppg_denoised, lowcut, highcut, fs)

# Apply Savitzky-Golay smoothing to the filtered signal
ppg_smoothed = savgol_filter(ppg_filtered, window_length=31, polyorder=1)

# Plotting the results
plt.figure(figsize=(12, 6))
plt.plot(ppg_smoothed, label='Final Smoothed Signal', color='red')
plt.title('Adaptive Wavelet Denoising with Bandpass Filtering and Smoothing')
plt.legend()
plt.xlabel('Sample')
plt.ylabel('Amplitude')
plt.grid(True)
plt.show()

from scipy.stats import skew, kurtosis, entropy

# Load your dataset (assuming it is saved as a CSV file)
df = pd.read_csv('subset_data_normalized.csv')

# Assuming the first 874 columns are time-series data, and the last two columns are labels
time_series_columns = df.columns[:-2]
label_columns = df.columns[-2:]

# Define a function to extract features from each row
def extract_features(row):
    signal = row[time_series_columns].values

    # Systolic and Diastolic Amplitude
    systolic_amplitude = np.max(signal)
    diastolic_amplitude = np.min(signal)

    # Augmentation Index (AIx)
    first_peak = systolic_amplitude  # Approximate as the max value
    second_peak = np.percentile(signal, 95)  # Second peak, approx as 95th percentile
    augmentation_index = (second_peak - first_peak) / first_peak if first_peak != 0 else 0

    # b/a Ratio
    b_a_ratio = second_peak / first_peak if first_peak != 0 else 0

    # Statistical Features
    mean_val = np.mean(signal)
    std_val = np.std(signal)
    skewness = skew(signal)
    kurt = kurtosis(signal)
    signal_entropy = entropy(np.abs(signal))
    rms = np.sqrt(np.mean(np.square(signal)))
    peak_to_peak = systolic_amplitude - diastolic_amplitude
    energy = np.sum(signal**2)

    # Combine all features into a dictionary
    features = {
        'systolic_amplitude': systolic_amplitude,
        'diastolic_amplitude': diastolic_amplitude,
        'augmentation_index': augmentation_index,
        'b_a_ratio': b_a_ratio,
        'mean': mean_val,
        'std_dev': std_val,
        'skewness': skewness,
        'kurtosis': kurt,
        'entropy': signal_entropy,
        'rms': rms,
        'peak_to_peak': peak_to_peak,
        'energy': energy,
    }
    return features

# Apply the feature extraction to each row and create a new DataFrame
features_df = df.apply(extract_features, axis=1, result_type='expand')

# Include labels in the new DataFrame
features_df[label_columns] = df[label_columns]

# Save the extracted features to a new CSV file
features_df.to_csv('normalised_extracted_features.csv', index=False)

print("Feature extraction completed. The features have been saved to 'normalised_extracted_features.csv'.")

normalised_extracted_features=pd.read_csv("normalised_extracted_features.csv")

normalised_extracted_features.head()

min_value = np.min(normalised_extracted_features)
max_value = np.max(normalised_extracted_features)

print("Min Value:", min_value)
print("Max Value:", max_value)

normalised_extracted_features_final= normalised_extracted_features.apply(lambda x: (x - x.min()) / (x.max() - x.min()), axis=0)

# Assuming 'data' is the numpy ndarray of your dataset
min_value = np.min(normalised_extracted_features_final)
max_value = np.max(normalised_extracted_features_final)

print("Min Value:", min_value)
print("Max Value:", max_value)

df22=normalised_extracted_features_final.drop(columns=['mean','std_dev'])
df22.info

from sklearn.model_selection import train_test_split
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score

# Load your dataset (df22)
# Assuming df22 is already loaded
# Take only 10,000 rows from the dataset
df_subset = df22.sample(n=100000, random_state=42)  # Randomly select 10,000 rows

# Split into features (X) and labels (y)
X = df_subset.iloc[:, :-2]  # All columns except the last two
y_systolic = df_subset.iloc[:, -2]  # Column '873'
y_diastolic = df_subset.iloc[:, -1]  # Column '874'

# Normalize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train_systolic, y_test_systolic = train_test_split(X_scaled, y_systolic, test_size=0.2, random_state=42)
X_train, X_test, y_train_diastolic, y_test_diastolic = train_test_split(X_scaled, y_diastolic, test_size=0.2, random_state=42)

# Train SVM models
svm_systolic = SVR(kernel='rbf')  # Using Radial Basis Function (RBF) kernel
svm_diastolic = SVR(kernel='rbf')

svm_systolic.fit(X_train, y_train_systolic)
svm_diastolic.fit(X_train, y_train_diastolic)

# Predict on test set
y_pred_systolic = svm_systolic.predict(X_test)
y_pred_diastolic = svm_diastolic.predict(X_test)

# Convert predictions to integer values for classification-like evaluation
# This assumes discretized labels. If not, the accuracy score won't be meaningful.
y_pred_systolic_discrete = np.round(y_pred_systolic)
y_pred_diastolic_discrete = np.round(y_pred_diastolic)

y_test_systolic_discrete = np.round(y_test_systolic)
y_test_diastolic_discrete = np.round(y_test_diastolic)

# Evaluate accuracy (discretized)
accuracy_systolic = accuracy_score(y_test_systolic_discrete, y_pred_systolic_discrete)
accuracy_diastolic = accuracy_score(y_test_diastolic_discrete, y_pred_diastolic_discrete)

print(f"Systolic Prediction - Accuracy: {accuracy_systolic:.4f}")
print(f"Diastolic Prediction - Accuracy: {accuracy_diastolic:.4f}")

# Evaluate MSE and R² for continuous predictions
mse_systolic = mean_squared_error(y_test_systolic, y_pred_systolic)
mse_diastolic = mean_squared_error(y_test_diastolic, y_pred_diastolic)

r2_systolic = r2_score(y_test_systolic, y_pred_systolic)
r2_diastolic = r2_score(y_test_diastolic, y_pred_diastolic)

print(f"Systolic Prediction - MSE: {mse_systolic:.4f}, R²: {r2_systolic:.4f}")
print(f"Diastolic Prediction - MSE: {mse_diastolic:.4f}, R²: {r2_diastolic:.4f}")

# Plot predictions vs actual values
import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))

# Systolic
plt.subplot(1, 2, 1)
plt.scatter(y_test_systolic, y_pred_systolic, alpha=0.6, color='blue', label='Predicted')
plt.plot(y_test_systolic, y_test_systolic, color='red', label='Ideal Fit Line')
plt.title('Systolic Prediction')
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.legend()

# Diastolic
plt.subplot(1, 2, 2)
plt.scatter(y_test_diastolic, y_pred_diastolic, alpha=0.6, color='green', label='Predicted')
plt.plot(y_test_diastolic, y_test_diastolic, color='red', label='Ideal Fit Line')
plt.title('Diastolic Prediction')
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.legend()

plt.tight_layout()
plt.show()

from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score

# Assuming df22 is your dataset
# Take only 50,000 rows
subset_data = normalised_extracted_features_final.iloc[:50000, :]

# Separate features (X) and labels (y)
X = subset_data.iloc[:, :-2].values  # All columns except the last two (labels)
y_systolic = subset_data.iloc[:, -2].values  # Column 873
y_diastolic = subset_data.iloc[:, -1].values  # Column 874

# Apply PCA to reduce dimensionality
n_components = 6 # Adjust the number of principal components as needed
pca = PCA(n_components=n_components)
X_pca = pca.fit_transform(X)

# Split into train and test sets
X_train, X_test, y_systolic_train, y_systolic_test = train_test_split(
    X_pca, y_systolic, test_size=0.3, random_state=42
)
X_train, X_test, y_diastolic_train, y_diastolic_test = train_test_split(
    X_pca, y_diastolic, test_size=0.3, random_state=42
)

# Train SVM for Systolic
svm_systolic = SVR(kernel='rbf', C=5, gamma=0.1)
svm_systolic.fit(X_train, y_systolic_train)

# Predict and evaluate for Systolic
y_systolic_pred = svm_systolic.predict(X_test)
systolic_mse = mean_squared_error(y_systolic_test, y_systolic_pred)
systolic_r2 = r2_score(y_systolic_test, y_systolic_pred)

# Train SVM for Diastolic
svm_diastolic = SVR(kernel='rbf', C=5, gamma=0.1)
svm_diastolic.fit(X_train, y_diastolic_train)

# Predict and evaluate for Diastolic
y_diastolic_pred = svm_diastolic.predict(X_test)
diastolic_mse = mean_squared_error(y_diastolic_test, y_diastolic_pred)
diastolic_r2 = r2_score(y_diastolic_test, y_diastolic_pred)

# Print results
print("Systolic Prediction - MSE:", systolic_mse, "R²:", systolic_r2)
print("Diastolic Prediction - MSE:", diastolic_mse, "R²:", diastolic_r2)

# Explained variance by PCA components
explained_variance = pca.explained_variance_ratio_
print("Explained Variance Ratio of PCA Components:", explained_variance)

accuracy_systolic = accuracy_score(y_test_systolic_discrete, y_pred_systolic_discrete)
accuracy_diastolic = accuracy_score(y_test_diastolic_discrete, y_pred_diastolic_discrete)

print(f"Systolic Prediction - Accuracy: {accuracy_systolic:.4f}")
print(f"Diastolic Prediction - Accuracy: {accuracy_diastolic:.4f}")

# apply ridge on -1 to 1

import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.svm import SVR
from sklearn.linear_model import Ridge, Lasso
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score

#KNN

from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt

# Assuming `normalised_extracted_features_final` is already defined
subset_data = normalised_extracted_features_final.iloc[: 50000, :]

# Separate features (X) and labels (y)
X = subset_data.iloc[:, :-2].values  # All columns except the last two (labels)
y_systolic = subset_data.iloc[:, -2].values  # Systolic (Column 873)
y_diastolic = subset_data.iloc[:, -1].values  # Diastolic (Column 874)

# Apply PCA to reduce dimensionality
n_components = 7  # Adjust the number of principal components as needed
pca = PCA(n_components=n_components)
X_pca = pca.fit_transform(X)

# Split into train and test sets
X_train, X_test, y_systolic_train, y_systolic_test = train_test_split(
    X_pca, y_systolic, test_size=0.2, random_state=42
)
X_train, X_test, y_diastolic_train, y_diastolic_test = train_test_split(
    X_pca, y_diastolic, test_size=0.2, random_state=42
)

# Define hyperparameter grid for GridSearchCV
param_grid = {
    "n_neighbors": np.arange(1, 55),
    "weights": ["uniform", "distance"],
    "metric": ["euclidean", "manhattan", "minkowski"]
}

# Grid Search for Systolic Prediction (KNN Regression)
knn_systolic = KNeighborsRegressor()
grid_search_systolic = GridSearchCV(
    estimator=knn_systolic,
    param_grid=param_grid,
    cv=4,
    scoring="neg_mean_squared_error",
    n_jobs=-1
)
grid_search_systolic.fit(X_train, y_systolic_train)
best_knn_systolic = grid_search_systolic.best_estimator_

# Evaluate Systolic Prediction
systolic_pred = best_knn_systolic.predict(X_test)
systolic_mse = mean_squared_error(y_systolic_test, systolic_pred)
systolic_r2 = r2_score(y_systolic_test, systolic_pred)

# Grid Search for Diastolic Prediction (KNN Regression)
knn_diastolic = KNeighborsRegressor()
grid_search_diastolic = GridSearchCV(
    estimator=knn_diastolic,
    param_grid=param_grid,
    cv=4,
    scoring="neg_mean_squared_error",
    n_jobs=-1
)
grid_search_diastolic.fit(X_train, y_diastolic_train)
best_knn_diastolic = grid_search_diastolic.best_estimator_

# Evaluate Diastolic Prediction
diastolic_pred = best_knn_diastolic.predict(X_test)
diastolic_mse = mean_squared_error(y_diastolic_test, diastolic_pred)
diastolic_r2 = r2_score(y_diastolic_test, diastolic_pred)

# Print Results
print("Systolic Prediction:")
print("Best Params:", grid_search_systolic.best_params_)
print(f"MSE: {systolic_mse:.4f}, R²: {systolic_r2:.4f}")

print("\nDiastolic Prediction:")
print("Best Params:", grid_search_diastolic.best_params_)
print(f"MSE: {diastolic_mse:.4f}, R²: {diastolic_r2:.4f}")

# Explained variance by PCA components
explained_variance = pca.explained_variance_ratio_
print("Explained Variance Ratio of PCA Components:", explained_variance)

# Plot Results
plt.figure(figsize=(14, 6))

# Systolic Scatter Plot
plt.subplot(1, 2, 1)
plt.scatter(y_systolic_test, systolic_pred, alpha=0.6, color='blue', label='Predicted')
plt.plot(y_systolic_test, y_systolic_test, color='red', label='Ideal Fit Line')
plt.title('Systolic Prediction')
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.legend()

# Diastolic Scatter Plot
plt.subplot(1, 2, 2)
plt.scatter(y_diastolic_test, diastolic_pred, alpha=0.6, color='green', label='Predicted')
plt.plot(y_diastolic_test, y_diastolic_test, color='red', label='Ideal Fit Line')
plt.title('Diastolic Prediction')
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.legend()

plt.tight_layout()
plt.show()









